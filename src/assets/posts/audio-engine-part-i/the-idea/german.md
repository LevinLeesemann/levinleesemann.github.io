Ein offensichtliches Problem von Anfang an war, dass alles, was gebaut werden würde, in die nativen Audiosysteme jeder Plattform integriert werden musste. Wer sich die Echtzeit-Bearbeitungs- und Wiedergabefunktionen von Gap Click und Beat Note anschaut, merkt schnell, dass die High-Level-Audio-APIs der Plattformen, wie zum Beispiel [Audio Queue Services](https://developer.apple.com/documentation/audiotoolbox/audio-queue-services) auf iOS, nicht ausreichen, da sie meist nur einfache Planungs- und Wiedergabefunktionen bieten. Letztendlich bedeutete das, dass ich auf iOS mit [Audio Unit](https://developer.apple.com/documentation/audiotoolbox/audio-unit-v2-c-api), auf Android mit [Oboe](https://github.com/google/oboe) und im Web mit der [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) arbeiten würde.

Wie bereits erwähnt, war eines der Hauptziele, die Duplizierung von Logik zwischen den Plattformen zu reduzieren. Sowohl Gap Click als auch Beat Note nutzen bereits [Kotlin Multiplatform](https://kotlinlang.org/docs/multiplatform.html) (KMP), um gemeinsame Logik einmal zu schreiben und sie dann von der nativen Codebasis jeder Plattform aus aufzurufen. Dieser Ansatz funktioniert gut für höherstufige Logik, aber Low-Level-Audio bringt strengere Anforderungen mit sich.

Ein zentraler Teil der Logik, der geteilt werden musste, ist das System, das steuert, wie Audio sequenziert, abgespielt und integriert wird, zum Beispiel für das Auslösen von Events. KMP wurde für diesen Zweck in Betracht gezogen, aber es gab ein paar Probleme.

Auf Android ist Oboe eine C++-API, die von Kotlin aus über JNI aufgerufen werden muss. Wichtiger noch: Echtzeit-Audio-Garantien sind in einer Garbage-Collected-Umgebung wie der JVM extrem schwer einzuhalten. Das macht es praktisch unmöglich, zeitkritische Audio-Logik ausschließlich in KMP zu implementieren. Web und iOS haben nicht genau dasselbe Problem, da sie dedizierte Audio-Threads nutzen. Nach ersten Untersuchungen schien die Integration von KMP-generierter Logik dort aber auch nicht trivial zu sein.

Die meisten Entwickler in diesem Bereich würden automatisch zu C++ greifen, da es der Standard für Low-Level-Audio ist. Es gab jedoch eine weitere Option, die in den letzten Jahren an Popularität gewonnen hat: Rust. Rust bietet sichereres Speicher-Management, exzellente Performance, in vielen Fällen bessere Entwickler-Ergonomie als C++ und eine C-ähnliche ABI-Integration (dazu später in den Abschnitten zu den nativen Layern mehr). Zum Zeitpunkt der Evaluierung war Rust in mainstream Audio-Apps noch kaum verbreitet, was es zu einer spannenden Gelegenheit machte, eine neue Sprache zu lernen und gleichzeitig in einem vertrauten Domänenbereich zu arbeiten.

Vielleicht fragt man sich, wie KMP noch ins Spiel passt, wenn Rust die gemeinsame Audio-Logik übernimmt. Das wird im Verlauf des Beitrags klarer werden. Auf hoher Ebene sieht der Stack jedoch so aus: KMP für die gemeinsame App-Logik (`platform`-Layer), ein Layer pro Zielplattform für die Audio-API-Integration (`native`-Layer) und Rust für die darunterliegende Echtzeit-Audio-Engine (`core`-Layer).

Der Stack sieht in etwa so aus:

![Architecture](/architecture-german.png)
