An obvious problem up front was that whatever ended up being built had to integrate with each platform's native audio system. If you take a quick look at the real-time editing and playback features that Gap Click and Beat Note provide, it becomes quite obvious that high-level audio APIs on each platform (such as [Audio Queue Services](https://developer.apple.com/documentation/audiotoolbox/audio-queue-services) on iOS) will not suffice, as they often only offer simple scheduling and playback controls. Ultimately, that meant I would be working with [Audio Unit](https://developer.apple.com/documentation/audiotoolbox/audio-unit-v2-c-api) on iOS, [Oboe](https://github.com/google/oboe) on Android, and the [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) on web.

As previously stated, one of the main goals was to reduce duplicated logic across platforms. Both Gap Click and Beat Note already handle some shared code using [Kotlin Multiplatform](https://kotlinlang.org/docs/multiplatform.html) (KMP), which lets you write common logic once and call it from each target's native code. This approach works well for higher-level logic, but low-level audio has some tighter requirements.

A key piece of logic that needed to be shared is the system that controls how audio is sequenced, played back, and integrated with (to emit events, for example). KMP was considered for this purpose, but there are a couple of problems.

On Android, Oboe is a C++ API (which in turn requires going through JNI when coming from Kotlin), and more importantly, real-time audio guarantees are extremely hard to meet in a garbage-collected environment like the JVM. This makes handling timing-sensitive audio logic purely in KMP a non-starter. Web and iOS don't quite have the same issues due to dedicated audio threads, but after some initial research, integrating KMP generated logic with them also appeared as though it wouldn't be super straightforward.

Most developers in this space would automatically reach for C++, since it's the standard for low-level audio work. But another option had been making the rounds in recent years: Rust. It provides safer memory management, excellent performance, arguably better ergonomics for developers than C++, and C-style ABI integration (this will be discussed a bit in each native layer's section). At the time of evaluation, Rust hadn't been used widely in mainstream audio apps, which made it an exciting choice for learning a new language, while also working in a domain I was already familiar with.

You might be wondering how KMP fits in if Rust is handling the shared audio logic. That will become apparent throughout the post, but at a high level, the stack looks like this: KMP for shared app logic (`platform` layer), a layer specific to each target for audio API integration (`native` layers), and Rust for the underlying real-time audio engine (`core` layer). It looks like this:

![Architecture](/architecture-english.png)
