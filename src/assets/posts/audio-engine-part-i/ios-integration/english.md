The iOS side of things was familiar territory for me, since I had already worked on it for my own metronome app. I had worked with the Audio Unit API before, so I had a reasonable idea for what the integration might look like. That didn't mean it was trivial, but at least I wasn't starting from scratch.

One of the first hurdles was getting Rust to play nicely with Xcode. I ended up using cbindgen to generate C headers from the Rust code, which worked well in principle, but getting it packaged with the compiled code into an Xcode-friendly library took some trial and error. The goal was to have something that never got in the way, and just plain works, regardless of whether the repo was freshly cloned or in-between builds.

Similar to the web setup, I built a couple of small test harness apps to experiment with the integration. One used SwiftUI and the other used UIKit, since both were realistic targets for the consuming applications. On iOS, the threading model is fairly straightforward: the producer runs on a DispatchQueue, while the audio thread is owned by the Audio Unit API. You provide your audio code through an AURenderCallback, and the system calls you when it needs more audio.

While working through this, I ran into one of those lessons that feels obvious in hindsight. Up until now I had been assuming that I could set the buffer size up front from the core, however this is usually just a request that you can make to each of the native Audio APIs once they're running. The more common approach is to have the native audio API determine the preferred buffer size, since that preference can change at runtime, even from one callback to the next. The dedicated audio thread usually receives this information as context, and it needs to flow back into the core engine so everything stays in sync.

That realization forced a small detour back to the web implementation to support buffer sizes determined by the native API, rather than assuming a fixed size. It was a good reminder that audio engines don't really get to be in charge. They have to adapt to whatever the platform and hardware decide is best at any given moment.

At this point, it was becoming clear that the native logic on iOS and the web was starting to look pretty similar between the two. Still, it wasn't quite at the point where it made sense to abstract everything behind a shared interface. I also wanted to hold off until Android was finished, since whatever constraints showed up there could easily change how the common pieces needed to be shaped.